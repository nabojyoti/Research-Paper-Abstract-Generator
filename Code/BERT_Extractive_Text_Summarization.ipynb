{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7kOx0267b-ve",
        "outputId": "45400aae-eccf-4691-9db6-5c5703eb1ce9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers==4.5.0 in /usr/local/lib/python3.7/dist-packages (4.5.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.64.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.0.53)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (21.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (4.11.3)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (0.10.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (1.21.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.5.0) (3.7.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.5.0) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.5.0) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.5.0) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.5.0) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting spacy==2.0.12\n",
            "  Using cached spacy-2.0.12-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (1.21.6)\n",
            "Collecting cymem<1.32,>=1.30\n",
            "  Using cached cymem-1.31.2-cp37-cp37m-linux_x86_64.whl\n",
            "Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (0.2.9)\n",
            "Collecting regex==2017.4.5\n",
            "  Using cached regex-2017.4.5-cp37-cp37m-linux_x86_64.whl\n",
            "Collecting preshed<2.0.0,>=1.0.0\n",
            "  Using cached preshed-1.0.1-cp37-cp37m-manylinux1_x86_64.whl (79 kB)\n",
            "Collecting thinc<6.11.0,>=6.10.3\n",
            "  Using cached thinc-6.10.3.tar.gz (1.2 MB)\n",
            "Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (0.9.6)\n",
            "Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (0.28.0)\n",
            "Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.7/dist-packages (from spacy==2.0.12) (5.3.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (2022.5.18.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.0.12) (2.10)\n",
            "Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.6.2)\n",
            "Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.4.7.1)\n",
            "Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.9.0.1)\n",
            "Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (1.10.11)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (4.64.0)\n",
            "Requirement already satisfied: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from thinc<6.11.0,>=6.10.3->spacy==2.0.12) (1.15.0)\n",
            "Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy==2.0.12) (0.11.2)\n",
            "Building wheels for collected packages: thinc\n",
            "  Building wheel for thinc (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for thinc\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for thinc\n",
            "Failed to build thinc\n",
            "Installing collected packages: cymem, preshed, thinc, regex, spacy\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 2.0.6\n",
            "    Uninstalling cymem-2.0.6:\n",
            "      Successfully uninstalled cymem-2.0.6\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 3.0.6\n",
            "    Uninstalling preshed-3.0.6:\n",
            "      Successfully uninstalled preshed-3.0.6\n",
            "  Attempting uninstall: thinc\n",
            "    Found existing installation: thinc 7.4.0\n",
            "    Uninstalling thinc-7.4.0:\n",
            "      Successfully uninstalled thinc-7.4.0\n",
            "    Running setup.py install for thinc ... \u001b[?25l\u001b[?25herror\n",
            "  Rolling back uninstall of thinc\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/thinc-7.4.0.dist-info/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~hinc-7.4.0.dist-info\n",
            "  Moving to /usr/local/lib/python3.7/dist-packages/thinc/\n",
            "   from /usr/local/lib/python3.7/dist-packages/~hinc\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-9qfxs3ks/thinc_60ffe84fcc51410e81d86ff2cf0ef2d5/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-9qfxs3ks/thinc_60ffe84fcc51410e81d86ff2cf0ef2d5/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-h2nq2fal/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/thinc Check the logs for full command output.\u001b[0m\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.7/dist-packages (0.10.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (1.0.2)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (2.2.4)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (from bert-extractive-summarizer) (4.5.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.21.6)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (3.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.28.0)\n",
            "Collecting preshed<3.1.0,>=3.0.2\n",
            "  Using cached preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.9.1)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (7.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (57.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (4.64.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (2.23.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (1.0.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.4.1)\n",
            "Collecting cymem<2.1.0,>=2.0.2\n",
            "  Using cached cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy->bert-extractive-summarizer) (0.9.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (3.8.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy->bert-extractive-summarizer) (4.2.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2022.5.18.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy->bert-extractive-summarizer) (3.0.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (3.7.0)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.10.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (21.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers->bert-extractive-summarizer) (0.0.53)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers->bert-extractive-summarizer) (3.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n",
            "Installing collected packages: cymem, preshed\n",
            "  Attempting uninstall: cymem\n",
            "    Found existing installation: cymem 1.31.2\n",
            "    Uninstalling cymem-1.31.2:\n",
            "      Successfully uninstalled cymem-1.31.2\n",
            "  Attempting uninstall: preshed\n",
            "    Found existing installation: preshed 1.0.1\n",
            "    Uninstalling preshed-1.0.1:\n",
            "      Successfully uninstalled preshed-1.0.1\n",
            "Successfully installed cymem-2.0.6 preshed-3.0.6\n"
          ]
        }
      ],
      "source": [
        "! pip install transformers==4.5.0\n",
        "! pip install spacy==2.0.12\n",
        "! pip install bert-extractive-summarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow==2.0.0\n",
        "!pip3 install --quiet tensorflow-hub\n",
        "!pip3 install --quiet seaborn\n",
        "!pip3 install sentencepiece\n",
        "!pip3 install tf_sentencepiece"
      ],
      "metadata": {
        "id": "I0K4f99LYvNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gyKfZQp4sSsV"
      },
      "outputs": [],
      "source": [
        "from summarizer import Summarizer,TransformerSummarizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUaHMSE8rODa",
        "outputId": "3f5b4845-67b1-4b32-9de4-0cdbd76391d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import re\n",
        "\n",
        "import glob\n",
        "#stop words\n",
        "from nltk.corpus import stopwords\n",
        "#tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from tqdm.auto import tqdm\n",
        "from drive.MyDrive.pyrouge import Rouge\n",
        "\n",
        "tqdm.pandas()"
      ],
      "metadata": {
        "id": "ZT4ry0bf3LIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Stop word removal function\n",
        "import gensim\n",
        "all_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "\n",
        "#from spacy.lang.en.stop_words import STOP_WORDS\n",
        "\n",
        "def stop_word_remove(sentence):\n",
        "    temp = [token for token in sentence.split() if token not in all_stopwords]\n",
        "    return ' '.join(word for word in temp)"
      ],
      "metadata": {
        "id": "55LITcQ73kS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/Parsed_Papers/Introduction/Introduction/S0167739X13001349.txt'"
      ],
      "metadata": {
        "id": "ERYuJQwA3nod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_paper(path):\n",
        "  f = open(path, 'r')\n",
        "  text = str(f.read().strip())\n",
        "  return text"
      ],
      "metadata": {
        "id": "yo3bJANT3qoP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t = read_paper(path)"
      ],
      "metadata": {
        "id": "bUJtLBvX3s9E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def bodyMain(text):\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = (text.encode('ascii','ignore')).decode(\"utf-8\")\n",
        "\n",
        "  #Extract body from the paper\n",
        "\n",
        "  body_main = re.findall(r'INTRODUCTION(.*?)REFERENCES', text, flags= re.I)[0]\n",
        "\n",
        "  # body = ' '.join(body_main.split())\n",
        "  # body = body.split(\".\")\n",
        "  return body_main"
      ],
      "metadata": {
        "id": "kuvjAYvp3vUO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = bodyMain(t)\n",
        "x"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "jw9sPanM3xtv",
        "outputId": "9871bd1e-5e83-42e1-e91d-238fc56df8cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'   Highly-threaded, many-core devices such as GPUs have gained popularity in the last decade; both NVIDIA and AMD manufacture general purpose GPUs that fall in this category. The important distinction between these machines and traditional multi-core machines is that these devices provide a large number of low-overhead hardware threads with low-overhead context switching between them; this fast context-switch mechanism is used to hide the memory access latency of transferring data from slow large (and often global) memory to fast, small (and typically local) memory. Researchers have designed algorithms to solve many interesting problems for these devices, such as GPU sorting or hashing [14], linear algebra [57], dynamic programming [8,9], graph algorithms [1013], and many other classic algorithms [14,15]. These projects generally report impressive gains in performance. These devices appear to be here to stay. While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis [1620], there are no systematic theoretical models to analyze the performance of programs on these machines. We are interested in analyzing and characterizing performance of algorithms on these highly-threaded, many-core machines in a more abstract, algorithmic, and systematic manner.  Theoretical analysis relies upon models that represent underlying assumptions; if a model does not capture the important aspects of target machines and programs, then the analysis is not predictive of real performance. Over the years, computer scientists have designed various models to capture important aspects of the machines that we use. The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) model [21], which we teach undergraduates in their first algorithms class. This model assumes that all operations, including memory accesses, take unit time. While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines. Aggarwal and Vitter proposed the Disk Access Machine (DAM) model [22] which counts the number of memory transfers from slow to fast memory instead of simply counting the number of memory accesses by the program. Therefore, it better captures the fact that modern machines have memory hierarchies and exploiting spatial and temporal locality on these machines can lead to better performance. There are also a number of other models that consider the memory access costs of sequential algorithms in different ways [2329].  For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) model [30], and there is a large body of work describing and analyzing algorithms in the PRAM model [31,32]. In the PRAM model, the algorithms complexity is analyzed in terms of its workthe time taken by the algorithm on 1 processor, and span (also called depth and critical-path length)the time taken by the algorithm on an infinite number of processors. Given a machine with                          P                       processors, a PRAM algorithm with work                          W                       and span                          S                       completes in                          max                                                     (                            W                            /                            P                            ,                            S                            )                                                time. The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time. For modern machines, however, this assumption seldom holds. Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines [3335], shared memory machines and multi-cores [3640], or the combination of the two [41,42].  All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines. While superficially highly-threaded, many-core machines such as GPUs are shared memory machines, their characteristics are very different from traditional multi-core or multiprocessor shared memory machines. The most important distinction between the multi-cores and highly-threaded, many-core machines is the number of threads per core. On multi-core machines, context switch cost is high, and most models nominally assume that only one (or a small constant number of) thread(s) are running on each machine and this thread blocks when there is a memory access. Therefore, many models consider the number of memory transfers from slow memory to fast memory as a performance measure, and algorithms are designed to minimize these, since memory transfers take a significant amount of time. In contrast, highly-threaded, many-core machines are explicitly designed to have a large number of threads per core and a fast context switching mechanism. Highly-threaded many-cores are explicitly designed to hide memory latency; if a thread stalls on a memory operation, some other thread can be scheduled in its place. In principle, the number of memory transfers does not matter as long as there are enough threads to hide their latency. Therefore, if there are enough threads, we should, in principle, be able to use PRAM algorithms on such machines, since we can ignore the effect of memory transfers which is exactly what PRAM model does.  However, the number of threads required to reach the point where one gets PRAM performance depends on both the algorithm and the hardware. Since no highly-threaded, many-core machine allows an infinite number of threads, it is important to understand both (1) how many threads does a particular algorithm need to achieve PRAM performance, and (2) how does an algorithm perform when it has fewer threads than required to get PRAM performance? In this paper, we attempt to characterize these properties of algorithms. To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs [3]. The problem is embarrassingly parallel, each set membership test is independent of every other membership test. Fig.1                       shows the performance of this application, varying the number of threads per processor core, for two distinct GPUs. For both machines, the pattern is quite similar, at low thread counts, the performance increases (roughly linearly) with the number of threads, up until a transition region, after which the performance no longer increases with increasing thread count. While the location of the transition region is different for distinct GPU models, this general pattern is found in many applications. Once sufficient threads are present, the PRAM model adequately describes the performance of the application and increasing the number of threads no longer helps.  In this work, we propose the Threaded Many-core Memory (TMM) model that captures the performance characteristics of these highly-threaded, many-core machines. This model explicitly models the large number of threads per processor and the memory latency to slow memory. Note that while we motivate this model for highly-threaded many-core machines with synchronous computations, in principle, it can be used in any system which has fast context switching and enough threads to hide memory latency. Typical examples of such machines include both NVIDIA and AMD/ATI GPUs and the YarcData uRiKA system. We do not try to model the Intel Xeon Phi, due to its limited use of threading for latency hiding. In contrast, its approach to hide memory latency is primarily based on strided memory access patterns associated with vector computation.  If the latency of transfers from slow memory to fast memory is small, or if the number of threads per processor is infinite, then this model generally provides the same analysis results as the PRAM analysis. It, however, provides more intuition. (1) Ideally, we want to get the PRAM performance for algorithms using the fewest number of threads possible, since threads do have overhead. This model can help us pick such algorithms. (2) It also captures the reality of when memory latency is large and the number of threads is large but finite. In particular, it can distinguish between algorithms that have the same PRAM analysis, but one may be better at hiding latency than another with a bounded number of threads.  This model is a high-level model meant to be generally applicable to a number of machines which allow a large number of threads with fast context switching. Therefore, it abstracts away many implementation details of either the machine or the algorithm. We also assume that the hardware provides 0-cost and perfect scheduling between threads. In addition, it also models the machine as having only 2 levels of memory. In particular, we model a slow global memory and fast local memory shared by one core group. In practice, these machines may have many levels of memory. However, we are interested in the interplay between the farthest level, since the latencies are the largest at that level, and therefore have the biggest impact on the performance. We expect that the model can be extended to also model other levels of the memory hierarchy.  We analyze 4 classic algorithms for the problem of computing All Pairs Shortest Paths (APSP) on a weighted graph in the TMM model [43]. We compare the analysis from this model with the PRAM analysis of these 4 algorithms to gain intuition about the usefulness of both our model and the PRAM model for analyzing performance of algorithms on highly-threaded, many-core machines. Our results validate the intuition that this model can provide more information than the PRAM model for the large latency, finite thread case. In particular, we compare these algorithms and find specific relationships between hardware parameters (latency, fast memory size, limits on number of threads) under which some algorithms are better than others even if they have the same PRAM cost.  Following the formal analysis, we assess the utility of the model by comparing empirically measured performance on an individual machine to that predicted by the model. For two of the APSP algorithms, we illustrate the impact of various individual parameters on performance, showing the effectiveness of the model at predicting measured performance.  This paper is organized as follows. Section 2 presents related work. Section 3 describes the TMM model. Section 4 provides the 4 shortest paths algorithms and their analysis in both the PRAM and TMM models. Section 5 provides the lessons learned from this model; in particular, we see that algorithms that have the same PRAM performance have different performance in the TMM model since they are better at hiding memory latency with fewer threads. Section 6 continues the discussion of lessons learned, concentrating on the effects of problem size. Section 7 shows performance measurements for a pair of the APSP algorithms executing on a commercial GPU, illustrating correspondence between model predictions and empirical measurements. Finally, Section 8 concludes.   RELATED WORK   In this section, we briefly review the related work. We first review the work on abstract models of computations for both sequential and parallel machines. We then review recent work on algorithms and performance analysis of GPUs which are the most common current instantiations of highly-threaded, many-core machines.  Many machine and memory models have been designed for various types of parallel and sequential machines. In an early work, Aggarwal etal. [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels. It differs from the RAM model by defining that access to location                          x                       takes                          log                         x                       time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms. The Block Transfer model (BT) [27] addresses this deficiency by defining that a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time. Alpern etal. propose the Memory Hierarchy (MH) Framework [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level. But there are too many parameters in this model that can obscure algorithm analysis. Thus, they simplified and reduced the MH parameters by putting forward a new Uniform Memory Hierarchy (UMH) model [28,29]. Later, an ideal-cache model was introduced in [23,24] allowing analysis of cache-oblivious algorithms that use asymptotically optimal amounts of work and move data asymptotically optimally among multiple levels of cache without the necessity of tuning program variables according to hardware configuration parameters.  In the parallel case, although widely used, the PRAM [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free. Quite different to PRAM, the Bulk-Synchronous Parallel (BSP) model [34] attempts to bridge theory and practice by allowing processors to work asynchronously, and it models latency and limited bandwidth for distributed memory machines without shared memory. Culler etal. [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead. It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing. Vitter etal. [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines. But this model assumes that processors are interconnected via sharing of internal memory.  More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs). Arge etal. [36] present the Parallel External Memory (PEM) model with                          P                       processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the                          P                       processors. Blelloch et al. [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip. Bender etal. [44] present a concurrent cache-oblivious model. Blelloch etal. [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies. Chowdhury etal. [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores. They in [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private                                                                                     L                                                                                       1                                                                            caches and a shared                                                                                     L                                                                                       2                                                                            cache. All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e.the explicit use of many threads for the purpose of hiding memory latency.  While there has not been much work on abstract machine models for highly-threaded, many-core machines, there has been a lot of recent work on designing calibrated performance models for particular instantiations of these machines such as NVIDIA GPUs. We review some of that work here. Liu etal. [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely. Their model incorporates the relationship between problem size and performance, but only targets their biosequence application. Govindaraju etal. [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops. It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general. Ryoo etal. [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications. They do not, however, consider memory latency and multiple conflicting performance indicators. Kothapalli etal. are the first to define a general GPU analytical performance model in [47]. They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing. Using a different approach, Hong etal. [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP). Meantime, Baghsorkhi etal. [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly. The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures. Zhang and Owens [15] present a quantitative performance model that characterizes an applications performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses. More recently, Sim etal. [48] develop a performance analysis framework that consists of an analytical model and profiling tools. The framework does a good job in performance diagnostics on case studies of real codes. Kim etal. [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters. Parakh etal. [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces. All of these efforts are mainly focused on the practical calibrated performance models. No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines.  The TMM model is meant to model the asymptotic performance of algorithms on highly-threaded, many-core machines. The model should abstract away the details of particular implementations so as to be applicable to many instantiations of these machines, while being particular enough to model the performance of algorithms on these machines with reasonable accuracy. In this section, we will describe the important characteristics of these highly-threaded, many-core architectures and our model for analyzing algorithms for these architectures.  The most important high-level characteristic of highly-threaded, many-core architectures is that they provide a large number of hardware threads and use fast and low-overhead context-switching in order to hide the memory access latency from slow global memory.  Highly-threaded, many-core architectures typically consist of a number of core groups, each containing a number of processors (or cores),                            1                                                                              1                            A core group can also have a single core.                          a fixed number of registers, and a fixed quantity of fast local on-chip memory shared within a core group. A large slow global memory is shared by all the core groups. Registers and local on-chip memory are the fastest to access, while accessing the global memory may potentially take 100s of cycles. The TMM model models these machines as having a memory hierarchy with two levels of memory: slow global memory and fast local memory. In addition, on most highly-threaded, many-core machines, data is transferred from slow to fast memory in chunks; instead of just transferring one word at a time, the hardware tries to transfer a large number of words during a memory transfer. The chunk can either be a cache line from hardware managed caches, or an explicitly-managed combined read from multiple threads. Since this characteristic of using high-bandwidth transfers in order to counter high latencies is common to most many-core machines (and even most multi-core machines), the TMM model captures the chunk size as one of its parameters.  These architectures support a large number of hardware threads, much larger than the number of cores. Cores on a single core group execute in synchronous style where groups of threads execute in lock-step. When a thread group executing on a core group stalls on a slow memory access, in theory, a context switch occurs and another thread group is scheduled on that core group. The abstract architecture is shown in Fig.2                         . Note that this architecture abstraction ignores a number of details about the physical machine, including thread grouping, scheduling, etc.  The TMM model captures the important characteristics of a highly-threaded, many-core architecture by using six parameters shown in Table1                         .                             L                          is the latency for accessing the slow memory (in our case, the global memory which is shared by all the core groups).                             P                          is the total number of cores (or processors) in the machine.                             C                          is the maximum chunk size; the number of words that can be read from slow memory to fast memory in one memory transfer. The parameter                             Z                          represents the size of fast local memory per core group and                             Q                          represents the number of cores per core group. As mentioned earlier, in some instantiations, a core group can have a single core. In this case, a many-core machine looks very much like a multi-core machine with a large number of low-overhead hardware threads. Note that we do not have a parameter for the number of core groups, that quantity is simply                             P                            /                            Q                         . Finally                             X                          is the hardware limit on the number of threads an algorithm is allowed to generate per core. This limit is enforced due to many different constraints, such as constraints on the number of registers each thread uses and an explicit constraint on the number of threads. We unify these constraints into one parameter.  In addition to the architecture parameters, we must also consider the parameters which are determined by the algorithm. We assume that the programmer has written a correct synchronous program and taken care to balance the workload across the core groups. These program parameters are shown in Table2                         .                                                                                              T                                                                                                1                                                                                     represents the work of the algorithm, that is, the total number of operations that the program must perform (including fast memory accesses).                                                                                             T                                                                                                                                                                                     represents the span of the algorithm, that is, the total number of operations on the critical path. These are similar to the analogous PRAM parameters of work and time (or depth or critical-path length).  Next, we come to program parameters that are specific to many-core programs.                             M                          represents the total number of global memory operations performed by the algorithm. Note that this is the total number of operations, not total number of accesses. Since many-core machines often transfer data in large chunks, multiple memory accesses can combine into one memory transfer. For instance, if the many-core machine has a hardware managed cache, and the program accesses data sequentially, then there is only one memory operation for                             C                          memory accesses; these will count as one when accounting for                             M                         .                             T                          is the number of threads created by the program per core. We assume that the work is perfectly distributed among cores. Therefore, the total number of threads in the system is                             T                            P                         . On highly-threaded, many-core architectures, thread switching is used to hide memory latency. Therefore, it is beneficial to create as many threads as possible. However, the maximum number of threads is limited by both the hardware and the program. The software limitation has to do with parallelism, the number of threads per core is limited by                             T                                                                                                                         T                                                                                                1                                                                                       /                                                           (                                                                                                      T                                                                                                                                                                                                                                        P                               )                                                     . The hardware limits                             T                                                        X                         . Finally, we have a parameter                             S                         , which is the local memory used per thread.                             S                          and                             T                          are related parameters, since there is a limited amount of local memory in the system. The number of threads per core is at most                             T                                                        Z                            /                                                           (                               Q                               S                               )                                                     .  The TMM model is a high-level abstract model, meant to be applicable to many instantiations of hardware platforms that feature a large number of threads with fast context switching and a hierarchical memory subsystem of at least two levels with a large memory latency gap in between. Typical examples of this set include NVIDIA GPUs, AMD/ATI GPUs, and the uRiKA machine from YarcData.  For NVIDIA GPUs, a number of streaming multiprocessors (core groups in our terminology) share the same global memory. On each of these core groups, there are a number of CUDA cores                            2                                                                              2                            CUDA (aka Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA.                          that share a fixed number of registers and on-chip (fast) memory shared among the cores of the core group. A fast hardware-supported context-switching mechanism enables a large number of threads to execute concurrently. Transfers between slow global memory and fast local memory can occur in chunks of at most 32 words; these chunks can only be created if the memory accesses are within a specified range. Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cache [51]. All these features are well captured in the TMM model. Streaming multiprocessors serve the same role as a core group, while CUDA cores are equivalent to the cores defined in TMM. The width of memory access                             C                          is 32 due to the coalescing of the threads in a warp. Global memory latency and size of on-chip shared memory/L1 cache are also depicted by                             L                          and                             Z                          respectively.  Considering AMD/ATI GPUs and taking Cypress, the codename for Radeon HD5800 series GPUs, as an example, the architecture is composed of 20 Single-InstructionMultiple-Data (SIMD) computation engines. In each SIMD engine, there are 16 Thread Processors (TP) and a 32kB Local Data Store (LDS). Every TP is arranged as a five-way or four-way Very Long Instruction Word (VLIW) processor, and consists of 5 Stream Cores (SC). Low context-switch threading is well supported, and every 64 threads are grouped into a wavefront executing the same instruction. Basically, the SIMD engine can naturally be modeled by core groups. Each SC is modeled as a core in TMM, summing up to 1600 cores totally. LDS is straightforwardly described by the fast local memory of TMM. The width of memory access                             C                          in TMM equals to the wavefront width of 64 for AMD/ATI GPUs.  The uRiKA system from YarcData is also a potential target for the TMM model. Based on the description from Alverson etal. [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O. Therefore, 128 defines parameter                             X                         , the hard limit of number of threads per processor. There can be up to 65,000 threads in a 512 processor system and over 1 million threads at the maximum system size of 8192 processors, so that the latencies are hidden by accommodating many remote memory '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract(text):\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = (text.encode('ascii','ignore')).decode(\"utf-8\")\n",
        "\n",
        "  #Extract highlights, body from body of the paper \n",
        "  highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHASES', text, flags = re.I)[0]\n",
        "  body_main = re.findall(r'INTRODUCTION(.*?)REFERENCES', text, flags= re.I)[0]"
      ],
      "metadata": {
        "id": "E-qPtEPV3z03"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def abstract(text):\n",
        "  text = re.sub(\"@&#\", \" \", text)\n",
        "  text = re.sub(\"\\n\", \" \", text)\n",
        "  text = ' '.join(text.split())\n",
        "  text = (text.encode('ascii','ignore')).decode(\"utf-8\")\n",
        "\n",
        "  abst = re.findall(r'ABSTRACT(.*?)INTRODUCTION', text, flags = re.I)[0]\n",
        "  # print(\"abs_main: \",abst)\n",
        "  return abst"
      ],
      "metadata": {
        "id": "Md66X8KY32hA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_paper(text):\n",
        "  # Removes unwanted characters, accounting for unicode characters\n",
        "    text = re.sub(\"@&#\", \" \", text)\n",
        "    text = re.sub(\"\\n\", \" \", text)\n",
        "    text = ' '.join(text.split())\n",
        "    text = (text.encode('ascii', 'ignore')).decode(\"utf-8\")\n",
        "    \n",
        "\n",
        "  # Extracting the highlights, body from the paper\n",
        "    highlights = re.findall(r'HIGHLIGHTS(.*?)KEYPHRASES', text,  flags = re.I)[0]\n",
        "    abst = re.findall(r'.*(?:abstract)(.*?)introduction', text, flags=re.I)[0]\n",
        "    body_main = re.findall(r'INTRODUCTION(.*?)REFERENCES', text,  flags = re.I)[0]\n",
        "    #body_main = re.findall(r'.*(?:abstract)(.*?)references', text, flags=re.I)[0]\n",
        "    \n",
        "    # Making a copy of the body, lowercasing body text, removing punctuations & extra spaces\n",
        "    dummy_body = body_main.lower()\n",
        "    dummy_body = re.sub('[^\\w\\s\\d\\.]','',dummy_body)\n",
        "    dummy_body = ' '.join(dummy_body.split())\n",
        "    dummy_body = dummy_body.split(\".\")\n",
        "\n",
        "  # Removing extra spaces from the body text, which will be preserved to produce summaries\n",
        "  # And splitting into sentences\n",
        "    body = ' '.join(body_main.split())\n",
        "    body = body.split(\".\")\n",
        "\n",
        "  # Removing sentences that are too short or too long, as they wouldn't make apt summary text\n",
        "    for i,x in enumerate(dummy_body):\n",
        "        if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
        "            \n",
        "            dummy_body.pop(i)\n",
        "            body.pop(i)\n",
        "            \n",
        "    # Making a copy of the abstract, lowercasing abstract text, removing punctuations & extra spaces\n",
        "    dummy_abst = abst.lower()\n",
        "    dummy_abst = re.sub('[^\\w\\s\\d\\.]','',dummy_abst)\n",
        "    dummy_abst = ' '.join(dummy_abst.split())\n",
        "    dummy_abst = dummy_abst.split(\".\")\n",
        "    \n",
        "  # Removing extra spaces from the abstract text, which will be preserved to produce summaries\n",
        "  # And splitting into sentences\n",
        "    ab = ' '.join(abst.split())\n",
        "    ab = abst.split(\".\")\n",
        "    \n",
        "  # Removing sentences that are too short or too long, as they wouldn't make apt summary text\n",
        "    for i,x in enumerate(dummy_abst):\n",
        "        if (len(x.split())) < 3 or (len(x.split())) > 15: \n",
        "            \n",
        "            dummy_abst.pop(i)\n",
        "            ab.pop(i)\n",
        "\n",
        "  # Making a copy of the highlights, lowercasing body text, removing punctuations & extra spaces\n",
        "    dummy_highlights = highlights.lower()\n",
        "    dummy_highlights = re.sub('[^\\w\\s\\d]','',dummy_highlights)\n",
        "    dummy_highlights = ' '.join(dummy_highlights.split())\n",
        "\n",
        "  # Removing stop words from body & highlights\n",
        "    body_copy = []\n",
        "    for x in dummy_body:\n",
        "      \n",
        "        body_copy.append(stop_word_remove(x))\n",
        "    highlight_copy = []\n",
        "    for x in dummy_highlights.split():\n",
        "        highlight_copy.append(stop_word_remove(x))\n",
        "        \n",
        "    # Removing stop words from abstarct\n",
        "    abst_copy = []\n",
        "    for x in dummy_abst:\n",
        "      \n",
        "        abst_copy.append(stop_word_remove(x))\n",
        "        \n",
        "    # Combing all of the abstract's sentences into one string    \n",
        "    abst_copy = \" \".join(sentence for sentence in abst_copy)\n",
        "    abst_copy = \" \".join(abst_copy.split())\n",
        "\n",
        "  \n",
        "  # Combing all of the highlights into one string    \n",
        "    highlight_copy = \" \".join(sentence for sentence in highlight_copy)\n",
        "    highlight_copy = \" \".join(highlight_copy.split())\n",
        "    #print(\"BC\",body_copy)\n",
        "    \n",
        "    return body_main, body_copy, highlights, highlight_copy, abst , abst_copy\n"
      ],
      "metadata": {
        "id": "AgQo9efv35ui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_main, body_copy, highlights, highlight_copy, abst , abst_copy = process_paper(t)"
      ],
      "metadata": {
        "id": "58AfHkEZ4DK3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "body_main"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "wU5hI0LS4FeK",
        "outputId": "58bc8200-0ea4-4944-8cd2-5eaf33d00737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' Highly-threaded, many-core devices such as GPUs have gained popularity in the last decade; both NVIDIA and AMD manufacture general purpose GPUs that fall in this category. The important distinction between these machines and traditional multi-core machines is that these devices provide a large number of low-overhead hardware threads with low-overhead context switching between them; this fast context-switch mechanism is used to hide the memory access latency of transferring data from slow large (and often global) memory to fast, small (and typically local) memory. Researchers have designed algorithms to solve many interesting problems for these devices, such as GPU sorting or hashing [14], linear algebra [57], dynamic programming [8,9], graph algorithms [1013], and many other classic algorithms [14,15]. These projects generally report impressive gains in performance. These devices appear to be here to stay. While there is a lot of folk wisdom on how to design good algorithms for these highly-threaded machines, in addition to a significant body of work on performance analysis [1620], there are no systematic theoretical models to analyze the performance of programs on these machines. We are interested in analyzing and characterizing performance of algorithms on these highly-threaded, many-core machines in a more abstract, algorithmic, and systematic manner. Theoretical analysis relies upon models that represent underlying assumptions; if a model does not capture the important aspects of target machines and programs, then the analysis is not predictive of real performance. Over the years, computer scientists have designed various models to capture important aspects of the machines that we use. The most fundamental model that is used to analyze sequential algorithms is the Random Access Machine (RAM) model [21], which we teach undergraduates in their first algorithms class. This model assumes that all operations, including memory accesses, take unit time. While this model is a good predictor of performance on computationally intensive programs, it does not properly capture the important characteristics of the memory hierarchy of modern machines. Aggarwal and Vitter proposed the Disk Access Machine (DAM) model [22] which counts the number of memory transfers from slow to fast memory instead of simply counting the number of memory accesses by the program. Therefore, it better captures the fact that modern machines have memory hierarchies and exploiting spatial and temporal locality on these machines can lead to better performance. There are also a number of other models that consider the memory access costs of sequential algorithms in different ways [2329]. For parallel computing, the analogue for the RAM model is the Parallel Random Access Machine (PRAM) model [30], and there is a large body of work describing and analyzing algorithms in the PRAM model [31,32]. In the PRAM model, the algorithms complexity is analyzed in terms of its workthe time taken by the algorithm on 1 processor, and span (also called depth and critical-path length)the time taken by the algorithm on an infinite number of processors. Given a machine with P processors, a PRAM algorithm with work W and span S completes in max ( W / P , S ) time. The PRAM model also ignores the vagaries of the memory hierarchy and assumes that each memory access by the algorithm takes unit time. For modern machines, however, this assumption seldom holds. Therefore, researchers have designed various models that capture memory hierarchies for various types of machines such as distributed memory machines [3335], shared memory machines and multi-cores [3640], or the combination of the two [41,42]. All of these models capture particular capabilities and properties of the respective target machines, namely shared memory machines or distributed memory machines. While superficially highly-threaded, many-core machines such as GPUs are shared memory machines, their characteristics are very different from traditional multi-core or multiprocessor shared memory machines. The most important distinction between the multi-cores and highly-threaded, many-core machines is the number of threads per core. On multi-core machines, context switch cost is high, and most models nominally assume that only one (or a small constant number of) thread(s) are running on each machine and this thread blocks when there is a memory access. Therefore, many models consider the number of memory transfers from slow memory to fast memory as a performance measure, and algorithms are designed to minimize these, since memory transfers take a significant amount of time. In contrast, highly-threaded, many-core machines are explicitly designed to have a large number of threads per core and a fast context switching mechanism. Highly-threaded many-cores are explicitly designed to hide memory latency; if a thread stalls on a memory operation, some other thread can be scheduled in its place. In principle, the number of memory transfers does not matter as long as there are enough threads to hide their latency. Therefore, if there are enough threads, we should, in principle, be able to use PRAM algorithms on such machines, since we can ignore the effect of memory transfers which is exactly what PRAM model does. However, the number of threads required to reach the point where one gets PRAM performance depends on both the algorithm and the hardware. Since no highly-threaded, many-core machine allows an infinite number of threads, it is important to understand both (1) how many threads does a particular algorithm need to achieve PRAM performance, and (2) how does an algorithm perform when it has fewer threads than required to get PRAM performance? In this paper, we attempt to characterize these properties of algorithms. To motivate this enterprise and to understand the importance of high thread counts on highly-threaded, many-core machines, let us consider a simple application that performs Bloom filter set membership tests on an input stream of biosequence data on GPUs [3]. The problem is embarrassingly parallel, each set membership test is independent of every other membership test. Fig. 1 shows the performance of this application, varying the number of threads per processor core, for two distinct GPUs. For both machines, the pattern is quite similar, at low thread counts, the performance increases (roughly linearly) with the number of threads, up until a transition region, after which the performance no longer increases with increasing thread count. While the location of the transition region is different for distinct GPU models, this general pattern is found in many applications. Once sufficient threads are present, the PRAM model adequately describes the performance of the application and increasing the number of threads no longer helps. In this work, we propose the Threaded Many-core Memory (TMM) model that captures the performance characteristics of these highly-threaded, many-core machines. This model explicitly models the large number of threads per processor and the memory latency to slow memory. Note that while we motivate this model for highly-threaded many-core machines with synchronous computations, in principle, it can be used in any system which has fast context switching and enough threads to hide memory latency. Typical examples of such machines include both NVIDIA and AMD/ATI GPUs and the YarcData uRiKA system. We do not try to model the Intel Xeon Phi, due to its limited use of threading for latency hiding. In contrast, its approach to hide memory latency is primarily based on strided memory access patterns associated with vector computation. If the latency of transfers from slow memory to fast memory is small, or if the number of threads per processor is infinite, then this model generally provides the same analysis results as the PRAM analysis. It, however, provides more intuition. (1) Ideally, we want to get the PRAM performance for algorithms using the fewest number of threads possible, since threads do have overhead. This model can help us pick such algorithms. (2) It also captures the reality of when memory latency is large and the number of threads is large but finite. In particular, it can distinguish between algorithms that have the same PRAM analysis, but one may be better at hiding latency than another with a bounded number of threads. This model is a high-level model meant to be generally applicable to a number of machines which allow a large number of threads with fast context switching. Therefore, it abstracts away many implementation details of either the machine or the algorithm. We also assume that the hardware provides 0-cost and perfect scheduling between threads. In addition, it also models the machine as having only 2 levels of memory. In particular, we model a slow global memory and fast local memory shared by one core group. In practice, these machines may have many levels of memory. However, we are interested in the interplay between the farthest level, since the latencies are the largest at that level, and therefore have the biggest impact on the performance. We expect that the model can be extended to also model other levels of the memory hierarchy. We analyze 4 classic algorithms for the problem of computing All Pairs Shortest Paths (APSP) on a weighted graph in the TMM model [43]. We compare the analysis from this model with the PRAM analysis of these 4 algorithms to gain intuition about the usefulness of both our model and the PRAM model for analyzing performance of algorithms on highly-threaded, many-core machines. Our results validate the intuition that this model can provide more information than the PRAM model for the large latency, finite thread case. In particular, we compare these algorithms and find specific relationships between hardware parameters (latency, fast memory size, limits on number of threads) under which some algorithms are better than others even if they have the same PRAM cost. Following the formal analysis, we assess the utility of the model by comparing empirically measured performance on an individual machine to that predicted by the model. For two of the APSP algorithms, we illustrate the impact of various individual parameters on performance, showing the effectiveness of the model at predicting measured performance. This paper is organized as follows. Section 2 presents related work. Section 3 describes the TMM model. Section 4 provides the 4 shortest paths algorithms and their analysis in both the PRAM and TMM models. Section 5 provides the lessons learned from this model; in particular, we see that algorithms that have the same PRAM performance have different performance in the TMM model since they are better at hiding memory latency with fewer threads. Section 6 continues the discussion of lessons learned, concentrating on the effects of problem size. Section 7 shows performance measurements for a pair of the APSP algorithms executing on a commercial GPU, illustrating correspondence between model predictions and empirical measurements. Finally, Section 8 concludes. RELATED WORK In this section, we briefly review the related work. We first review the work on abstract models of computations for both sequential and parallel machines. We then review recent work on algorithms and performance analysis of GPUs which are the most common current instantiations of highly-threaded, many-core machines. Many machine and memory models have been designed for various types of parallel and sequential machines. In an early work, Aggarwal et al. [25] present the Hierarchical Memory Model (HMM) and use it for a theoretical investigation of the inherent complexity of solving problems in RAM with a memory hierarchy of multiple levels. It differs from the RAM model by defining that access to location x takes log x time, but it does not consider the concept of block transfers, which collects data into blocks to utilize spatial locality of reference in algorithms. The Block Transfer model (BT) [27] addresses this deficiency by defining that a block of consecutive locations can be copied from memory to memory, taking one unit of time per element after the initial access time. Alpern et al. propose the Memory Hierarchy (MH) Framework [26] that reflects important practical considerations that are hidden by the RAM and HMM models: data are moved in fixed size blocks simultaneously at different levels in the hierarchy, and the memory capacity as well as bus bandwidth are limited at each level. But there are too many parameters in this model that can obscure algorithm analysis. Thus, they simplified and reduced the MH parameters by putting forward a new Uniform Memory Hierarchy (UMH) model [28,29]. Later, an ideal-cache model was introduced in [23,24] allowing analysis of cache-oblivious algorithms that use asymptotically optimal amounts of work and move data asymptotically optimally among multiple levels of cache without the necessity of tuning program variables according to hardware configuration parameters. In the parallel case, although widely used, the PRAM [30] model is unrealistic because it assumes all processors work synchronously and that interprocessor communication is free. Quite different to PRAM, the Bulk-Synchronous Parallel (BSP) model [34] attempts to bridge theory and practice by allowing processors to work asynchronously, and it models latency and limited bandwidth for distributed memory machines without shared memory. Culler et al. [33] offer a new parallel machine model called LogP based on BSP, characterizing a parallel machine by four parameters: number of processors, communication bandwidth, delay, and overhead. It reflects the convergence towards systems formed by a collection of computers connected by a communication network via message passing. Vitter et al. [35] present a two-level memory model and give a realistic treatment of parallel block transfers in parallel machines. But this model assumes that processors are interconnected via sharing of internal memory. More recently, several models have been proposed emphasizing the use of private-cache chip multiprocessors (CMPs). Arge et al. [36] present the Parallel External Memory (PEM) model with P processors and a two-level memory hierarchy, consisting of the main memory as external memory shared by all processors and caches as internal memory exclusive to each of the P processors. Blelloch et al. [37] present a multi-core-cache model capturing the fact that multi-core machines have both per-processor private caches and a large shared cache on-chip. Bender et al. [44] present a concurrent cache-oblivious model. Blelloch et al. [38] also propose a parallel cache-oblivious (PCO) model to account for costs of a wide range of cache hierarchies. Chowdhury et al. [39] present a hierarchical multi-level caching model (HM), consisting of a collection of cores sharing an arbitrarily large main memory through a hierarchy of caches of finite but increasing sizes that are successively shared by larger groups of cores. They in [42] consider three types of caching systems for CMPs: D-CMP with a private cache for each core, S-CMP with a single cache shared by all cores, and multi-core with private L 1 caches and a shared L 2 cache. All the models above do not accurately describe highly-threaded, many-core systems, due to their distinctive architectures, i.e. the explicit use of many threads for the purpose of hiding memory latency. While there has not been much work on abstract machine models for highly-threaded, many-core machines, there has been a lot of recent work on designing calibrated performance models for particular instantiations of these machines such as NVIDIA GPUs. We review some of that work here. Liu et al. [19] describe a general performance model that predicts the performance of a biosequence database scanning application fairly precisely. Their model incorporates the relationship between problem size and performance, but only targets their biosequence application. Govindaraju et al. [45] propose a cache model for efficiently implementing three memory intensive scientific applications with nested loops. It is helpful for applications with 2D-block representations while choosing an appropriate block size by estimating cache misses, but is not completely general. Ryoo et al. [46] summarize five categories of optimization mechanisms, and use two metrics to prune the GPU performance optimization space by 98% via computing the utilization and efficiency of GPU applications. They do not, however, consider memory latency and multiple conflicting performance indicators. Kothapalli et al. are the first to define a general GPU analytical performance model in [47]. They propose a simple yet efficient solution combining several well-known parallel computation models: PRAM, BSP, QRQW, but they do not model global memory coalescing. Using a different approach, Hong et al. [17] propose another analytical model to capture the cost of memory operations by counting the number of parallel memory requests in terms of memory-warp parallelism (MWP) and computation-warp parallelism (CWP). Meantime, Baghsorkhi et al. [16] measure performance factors in isolation and later combine them to model the overall performance via workflow graphs so that the interactive effects between different performance factors are modeled correctly. The model can determine data access patterns, branch divergence, and control flow patterns only for a restricted class of kernels on traditional GPU architectures. Zhang and Owens [15] present a quantitative performance model that characterizes an applications performance as being primarily bounded by one of three potential limits: instruction pipeline, shared memory accesses, and global memory accesses. More recently, Sim et al. [48] develop a performance analysis framework that consists of an analytical model and profiling tools. The framework does a good job in performance diagnostics on case studies of real codes. Kim et al. [49] also design a tool to estimate GPU memory performance by collecting performance-critical parameters. Parakh et al. [50] present a model to estimate both computation time by precisely counting instructions and memory access time by a method to generate address traces. All of these efforts are mainly focused on the practical calibrated performance models. No attempts have been made to develop an asymptotic theoretical model applicable to a wide range of highly-threaded machines. The TMM model is meant to model the asymptotic performance of algorithms on highly-threaded, many-core machines. The model should abstract away the details of particular implementations so as to be applicable to many instantiations of these machines, while being particular enough to model the performance of algorithms on these machines with reasonable accuracy. In this section, we will describe the important characteristics of these highly-threaded, many-core architectures and our model for analyzing algorithms for these architectures. The most important high-level characteristic of highly-threaded, many-core architectures is that they provide a large number of hardware threads and use fast and low-overhead context-switching in order to hide the memory access latency from slow global memory. Highly-threaded, many-core architectures typically consist of a number of core groups, each containing a number of processors (or cores), 1 1 A core group can also have a single core. a fixed number of registers, and a fixed quantity of fast local on-chip memory shared within a core group. A large slow global memory is shared by all the core groups. Registers and local on-chip memory are the fastest to access, while accessing the global memory may potentially take 100s of cycles. The TMM model models these machines as having a memory hierarchy with two levels of memory: slow global memory and fast local memory. In addition, on most highly-threaded, many-core machines, data is transferred from slow to fast memory in chunks; instead of just transferring one word at a time, the hardware tries to transfer a large number of words during a memory transfer. The chunk can either be a cache line from hardware managed caches, or an explicitly-managed combined read from multiple threads. Since this characteristic of using high-bandwidth transfers in order to counter high latencies is common to most many-core machines (and even most multi-core machines), the TMM model captures the chunk size as one of its parameters. These architectures support a large number of hardware threads, much larger than the number of cores. Cores on a single core group execute in synchronous style where groups of threads execute in lock-step. When a thread group executing on a core group stalls on a slow memory access, in theory, a context switch occurs and another thread group is scheduled on that core group. The abstract architecture is shown in Fig. 2 . Note that this architecture abstraction ignores a number of details about the physical machine, including thread grouping, scheduling, etc. The TMM model captures the important characteristics of a highly-threaded, many-core architecture by using six parameters shown in Table 1 . L is the latency for accessing the slow memory (in our case, the global memory which is shared by all the core groups). P is the total number of cores (or processors) in the machine. C is the maximum chunk size; the number of words that can be read from slow memory to fast memory in one memory transfer. The parameter Z represents the size of fast local memory per core group and Q represents the number of cores per core group. As mentioned earlier, in some instantiations, a core group can have a single core. In this case, a many-core machine looks very much like a multi-core machine with a large number of low-overhead hardware threads. Note that we do not have a parameter for the number of core groups, that quantity is simply P / Q . Finally X is the hardware limit on the number of threads an algorithm is allowed to generate per core. This limit is enforced due to many different constraints, such as constraints on the number of registers each thread uses and an explicit constraint on the number of threads. We unify these constraints into one parameter. In addition to the architecture parameters, we must also consider the parameters which are determined by the algorithm. We assume that the programmer has written a correct synchronous program and taken care to balance the workload across the core groups. These program parameters are shown in Table 2 . T 1 represents the work of the algorithm, that is, the total number of operations that the program must perform (including fast memory accesses). T  represents the span of the algorithm, that is, the total number of operations on the critical path. These are similar to the analogous PRAM parameters of work and time (or depth or critical-path length). Next, we come to program parameters that are specific to many-core programs. M represents the total number of global memory operations performed by the algorithm. Note that this is the total number of operations, not total number of accesses. Since many-core machines often transfer data in large chunks, multiple memory accesses can combine into one memory transfer. For instance, if the many-core machine has a hardware managed cache, and the program accesses data sequentially, then there is only one memory operation for C memory accesses; these will count as one when accounting for M . T is the number of threads created by the program per core. We assume that the work is perfectly distributed among cores. Therefore, the total number of threads in the system is T P . On highly-threaded, many-core architectures, thread switching is used to hide memory latency. Therefore, it is beneficial to create as many threads as possible. However, the maximum number of threads is limited by both the hardware and the program. The software limitation has to do with parallelism, the number of threads per core is limited by T  T 1 / ( T   P ) . The hardware limits T  X . Finally, we have a parameter S , which is the local memory used per thread. S and T are related parameters, since there is a limited amount of local memory in the system. The number of threads per core is at most T  Z / ( Q S ) . The TMM model is a high-level abstract model, meant to be applicable to many instantiations of hardware platforms that feature a large number of threads with fast context switching and a hierarchical memory subsystem of at least two levels with a large memory latency gap in between. Typical examples of this set include NVIDIA GPUs, AMD/ATI GPUs, and the uRiKA machine from YarcData. For NVIDIA GPUs, a number of streaming multiprocessors (core groups in our terminology) share the same global memory. On each of these core groups, there are a number of CUDA cores 2 2 CUDA (aka Compute Unified Device Architecture) is a parallel computing platform and programming model created by NVIDIA. that share a fixed number of registers and on-chip (fast) memory shared among the cores of the core group. A fast hardware-supported context-switching mechanism enables a large number of threads to execute concurrently. Transfers between slow global memory and fast local memory can occur in chunks of at most 32 words; these chunks can only be created if the memory accesses are within a specified range. Accessing the off-chip global memory usually takes 20 to 40 times more clock cycles than accessing the on-chip shared memory/L1 cache [51]. All these features are well captured in the TMM model. Streaming multiprocessors serve the same role as a core group, while CUDA cores are equivalent to the cores defined in TMM. The width of memory access C is 32 due to the coalescing of the threads in a warp. Global memory latency and size of on-chip shared memory/L1 cache are also depicted by L and Z respectively. Considering AMD/ATI GPUs and taking Cypress, the codename for Radeon HD5800 series GPUs, as an example, the architecture is composed of 20 Single-InstructionMultiple-Data (SIMD) computation engines. In each SIMD engine, there are 16 Thread Processors (TP) and a 32 kB Local Data Store (LDS). Every TP is arranged as a five-way or four-way Very Long Instruction Word (VLIW) processor, and consists of 5 Stream Cores (SC). Low context-switch threading is well supported, and every 64 threads are grouped into a wavefront executing the same instruction. Basically, the SIMD engine can naturally be modeled by core groups. Each SC is modeled as a core in TMM, summing up to 1600 cores totally. LDS is straightforwardly described by the fast local memory of TMM. The width of memory access C in TMM equals to the wavefront width of 64 for AMD/ATI GPUs. The uRiKA system from YarcData is also a potential target for the TMM model. Based on the description from Alverson et al. [52] about the nature of the computations this processor was designed to run, it is a purpose-built appliance for real-time graph analytics featuring graph-optimized hardware that provides up to 512 terabytes of global shared memory, massively-multi-threaded graph processors (named Threadstorm) supporting 128 threads/processor, and highly scalable I/O. Therefore, 128 defines parameter X , the hard limit of number of threads per processor. There can be up to 65,000 threads in a 512 processor system and over 1 million threads at the maximum system size of 8192 processors, so that the latencies are hidden by accommodating many remote memory '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "body_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WjDe-cB4GlI",
        "outputId": "6de932f6-d82c-4cdd-f05a-e4b0995b8713"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['important distinction machines traditional multicore machines devices provide large number lowoverhead hardware threads lowoverhead context switching fast contextswitch mechanism hide memory access latency transferring data slow large global memory fast small typically local memory',\n",
              " 'projects generally report impressive gains performance',\n",
              " 'devices appear stay',\n",
              " 'interested analyzing characterizing performance algorithms highlythreaded manycore machines abstract algorithmic systematic manner',\n",
              " 'years scientists designed models capture important aspects machines use',\n",
              " 'model assumes operations including memory accesses unit time',\n",
              " 'aggarwal vitter proposed disk access machine dam model 22 counts number memory transfers slow fast memory instead simply counting number memory accesses program',\n",
              " 'number models consider memory access costs sequential algorithms different ways 2329',\n",
              " 'pram model algorithms complexity analyzed terms workthe time taken algorithm 1 processor span called depth criticalpath lengththe time taken algorithm infinite number processors',\n",
              " 'pram model ignores vagaries memory hierarchy assumes memory access algorithm takes unit time',\n",
              " 'modern machines assumption seldom holds',\n",
              " 'models capture particular capabilities properties respective target machines shared memory machines distributed memory machines',\n",
              " 'important distinction multicores highlythreaded manycore machines number threads core',\n",
              " 'models consider number memory transfers slow memory fast memory performance measure algorithms designed minimize memory transfers significant time',\n",
              " 'highlythreaded manycores explicitly designed hide memory latency thread stalls memory operation thread scheduled place',\n",
              " 'threads principle able use pram algorithms machines ignore effect memory transfers exactly pram model',\n",
              " 'highlythreaded manycore machine allows infinite number threads important understand 1 threads particular algorithm need achieve pram performance 2 algorithm perform fewer threads required pram performance paper attempt characterize properties algorithms',\n",
              " 'problem embarrassingly parallel set membership test independent membership test',\n",
              " '1 shows performance application varying number threads processor core distinct gpus',\n",
              " 'location transition region different distinct gpu models general pattern applications',\n",
              " 'work propose threaded manycore memory tmm model captures performance characteristics highlythreaded manycore machines',\n",
              " 'note motivate model highlythreaded manycore machines synchronous computations principle fast context switching threads hide memory latency',\n",
              " 'try model intel xeon phi limited use threading latency hiding',\n",
              " 'latency transfers slow memory fast memory small number threads processor infinite model generally provides analysis results pram analysis',\n",
              " 'provides intuition',\n",
              " 'model help pick algorithms',\n",
              " 'particular distinguish algorithms pram analysis better hiding latency bounded number threads',\n",
              " 'abstracts away implementation details machine algorithm',\n",
              " 'assume hardware provides 0cost perfect scheduling threads',\n",
              " 'addition models machine having 2 levels memory',\n",
              " 'practice machines levels memory',\n",
              " 'expect model extended model levels memory hierarchy',\n",
              " 'compare analysis model pram analysis 4 algorithms gain intuition usefulness model pram model analyzing performance algorithms highlythreaded manycore machines',\n",
              " 'particular compare algorithms specific relationships hardware parameters latency fast memory size limits number threads algorithms better pram cost',\n",
              " 'apsp algorithms illustrate impact individual parameters performance showing effectiveness model predicting measured performance',\n",
              " 'paper organized follows',\n",
              " 'section 2 presents related work',\n",
              " 'section 3 describes tmm model',\n",
              " 'section 5 provides lessons learned model particular algorithms pram performance different performance tmm model better hiding memory latency fewer threads',\n",
              " 'section 6 continues discussion lessons learned concentrating effects problem size',\n",
              " 'finally section 8 concludes',\n",
              " 'related work section briefly review related work',\n",
              " 'review recent work algorithms performance analysis gpus common current instantiations highlythreaded manycore machines',\n",
              " 'early work aggarwal et al',\n",
              " 'differs ram model defining access location x takes log x time consider concept block transfers collects data blocks utilize spatial locality reference algorithms',\n",
              " 'alpern et al',\n",
              " 'parameters model obscure algorithm analysis',\n",
              " 'later idealcache model introduced 2324 allowing analysis cacheoblivious algorithms use asymptotically optimal amounts work data asymptotically optimally multiple levels cache necessity tuning program variables according hardware configuration parameters',\n",
              " 'different pram bulksynchronous parallel bsp model 34 attempts bridge theory practice allowing processors work asynchronously models latency limited bandwidth distributed memory machines shared memory',\n",
              " 'culler et al',\n",
              " 'reflects convergence systems formed collection computers connected communication network message passing',\n",
              " 'vitter et al',\n",
              " 'model assumes processors interconnected sharing internal memory',\n",
              " 'recently models proposed emphasizing use privatecache chip multiprocessors cmps',\n",
              " 'arge et al',\n",
              " 'blelloch et al',\n",
              " 'bender et al',\n",
              " '44 present concurrent cacheoblivious model',\n",
              " 'blelloch et al',\n",
              " 'chowdhury et al',\n",
              " '42 consider types caching systems cmps dcmp private cache core scmp single cache shared cores multicore private l 1 caches shared l 2 cache',\n",
              " 'e',\n",
              " 'explicit use threads purpose hiding memory latency',\n",
              " 'review work',\n",
              " 'liu et al',\n",
              " 'model incorporates relationship problem size performance targets biosequence application',\n",
              " 'govindaraju et al',\n",
              " 'helpful applications 2dblock representations choosing appropriate block size estimating cache misses completely general',\n",
              " 'ryoo et al',\n",
              " 'consider memory latency multiple conflicting performance indicators',\n",
              " 'kothapalli et al',\n",
              " 'define general gpu analytical performance model 47',\n",
              " 'different approach hong et al',\n",
              " 'meantime baghsorkhi et al',\n",
              " 'model determine data access patterns branch divergence control flow patterns restricted class kernels traditional gpu architectures',\n",
              " 'recently sim et al',\n",
              " '48 develop performance analysis framework consists analytical model profiling tools',\n",
              " 'framework good job performance diagnostics case studies real codes',\n",
              " 'kim et al',\n",
              " '49 design tool estimate gpu memory performance collecting performancecritical parameters',\n",
              " 'parakh et al',\n",
              " 'efforts mainly focused practical calibrated performance models',\n",
              " 'tmm model meant model asymptotic performance algorithms highlythreaded manycore machines',\n",
              " 'section important characteristics highlythreaded manycore architectures model analyzing algorithms architectures',\n",
              " 'highlythreaded manycore architectures typically consist number core groups containing number processors cores 1 1 core group single core',\n",
              " 'large slow global memory shared core groups',\n",
              " 'tmm model models machines having memory hierarchy levels memory slow global memory fast local memory',\n",
              " 'chunk cache line hardware managed caches explicitlymanaged combined read multiple threads',\n",
              " 'architectures support large number hardware threads larger number cores',\n",
              " 'thread group executing core group stalls slow memory access theory context switch occurs thread group scheduled core group',\n",
              " 'abstract architecture shown fig',\n",
              " 'note architecture abstraction ignores number details physical machine including thread grouping scheduling',\n",
              " 'l latency accessing slow memory case global memory shared core groups',\n",
              " 'p total number cores processors machine',\n",
              " 'parameter z represents size fast local memory core group q represents number cores core group',\n",
              " 'mentioned earlier instantiations core group single core',\n",
              " 'note parameter number core groups quantity simply p q',\n",
              " 'limit enforced different constraints constraints number registers thread uses explicit constraint number threads',\n",
              " 'unify constraints parameter',\n",
              " 'assume programmer written correct synchronous program taken care balance workload core groups',\n",
              " 'program parameters shown table 2',\n",
              " 't represents span algorithm total number operations critical path',\n",
              " 'come program parameters specific manycore programs',\n",
              " 'm represents total number global memory operations performed algorithm',\n",
              " 'note total number operations total number accesses',\n",
              " 'instance manycore machine hardware managed cache program accesses data sequentially memory operation c memory accesses count accounting m',\n",
              " 't number threads created program core',\n",
              " 'assume work perfectly distributed cores',\n",
              " 'total number threads t p',\n",
              " 'highlythreaded manycore architectures thread switching hide memory latency',\n",
              " 'beneficial create threads possible',\n",
              " 'maximum number threads limited hardware program',\n",
              " 'hardware limits t x',\n",
              " 'finally parameter s local memory thread',\n",
              " 'number threads core t z q s',\n",
              " 'typical examples set include nvidia gpus amdati gpus urika machine yarcdata',\n",
              " 'core groups number cuda cores 2 2 cuda aka compute unified device architecture parallel computing platform programming model created nvidia',\n",
              " 'fast hardwaresupported contextswitching mechanism enables large number threads execute concurrently',\n",
              " 'accessing offchip global memory usually takes 20 40 times clock cycles accessing onchip shared memoryl1 cache 51',\n",
              " 'features captured tmm model',\n",
              " 'width memory access c 32 coalescing threads warp',\n",
              " 'considering amdati gpus taking cypress codename radeon hd5800 series gpus example architecture composed 20 singleinstructionmultipledata simd computation engines',\n",
              " 'tp arranged fiveway fourway long instruction word vliw processor consists 5 stream cores sc',\n",
              " 'basically simd engine naturally modeled core groups',\n",
              " 'sc modeled core tmm summing 1600 cores totally',\n",
              " 'lds straightforwardly described fast local memory tmm',\n",
              " 'urika yarcdata potential target tmm model',\n",
              " 'based description alverson et al',\n",
              " '128 defines parameter x hard limit number threads processor']"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlights"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "M6FebjUk4Hkb",
        "outputId": "797cc33e-9728-4baa-a77f-3ebe275edc5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' We design a memory model to analyze algorithms for highly-threaded many-core systems. The model captures significant factors of performance: work, span, and memory accesses. We show the model is better than PRAM by applying both to 4 shortest paths algorithms. Empirical performance is effectively predicted by our model in many circumstances. It is the first formalized asymptotic model helpful for algorithm design on many-cores. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "highlight_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "JksFekUv4LeV",
        "outputId": "55ac9907-ac02-47a6-85e5-d8cb82fd1d29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'design memory model analyze algorithms highlythreaded manycore systems model captures significant factors performance work span memory accesses model better pram applying 4 shortest paths algorithms empirical performance effectively predicted model circumstances formalized asymptotic model helpful algorithm design manycores'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "YnHc23Gx4M4o",
        "outputId": "8345ea17-a5be-4f41-c7f3-6eb303e5be95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' A number of highly-threaded, many-core architectures hide memory-access latency by low-overhead context switching among a large number of threads. The speedup of a program on these machines depends on how well the latency is hidden. If the number of threads were infinite, theoretically, these machines could provide the performance predicted by the PRAM analysis of these programs. However, the number of threads per processor is not infinite, and is constrained by both hardware and algorithmic limits. In this paper, we introduce the Threaded Many-core Memory (TMM) model which is meant to capture the important characteristics of these highly-threaded, many-core machines. Since we model some important machine parameters of these machines, we expect analysis under this model to provide a more fine-grained and accurate performance prediction than the PRAM analysis. We analyze 4 algorithms for the classic all pairs shortest paths problem under this model. We find that even when two algorithms have the same PRAM performance, our model predicts different performance for some settings of machine parameters. For example, for dense graphs, the dynamic programming algorithm and Johnsons algorithm have the same performance in the PRAM model. However, our model predicts different performance for large enough memory-access latency and validates the intuition that the dynamic programming algorithm performs better on these machines. We validate several predictions made by our model using empirical measurements on an instantiation of a highly-threaded, many-core machine, namely the NVIDIA GTX 480. '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "abst_copy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 91
        },
        "id": "L6mw3mck4Nme",
        "outputId": "67944238-198a-4da7-b4c5-289b85dfb44b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'speedup program machines depends latency hidden number threads processor infinite constrained hardware algorithmic limits model important machine parameters machines expect analysis model provide finegrained accurate performance prediction pram analysis analyze 4 algorithms classic pairs shortest paths problem model example dense graphs dynamic programming algorithm johnsons algorithm performance pram model validate predictions model empirical measurements instantiation highlythreaded manycore machine nvidia gtx 480'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **summary**"
      ],
      "metadata": {
        "id": "PQPl7ssOBl6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test = glob.glob(f\"/content/drive/MyDrive/Parsed_Papers/Introduction/Test_Files/*.txt\")"
      ],
      "metadata": {
        "id": "PnpiYf8oBkWc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cp1PLjN9Brfw",
        "outputId": "bcf95d75-c7f8-431b-a621-27a34dbfb632"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "col_names = ['FileName','Abstract','Summary','Precision','Recall','F1_Score']\n",
        "\n",
        "result = pd.DataFrame(columns=col_names,dtype=object)\n",
        "\n",
        "for i,path in enumerate(tqdm(test[:2])):\n",
        "\n",
        "  file_name = path.split('/')[-1].replace('.txt', '')\n",
        "  text = read_paper(path)\n",
        "  abstractL = abstract(text)\n",
        "  body_main = bodyMain(text)\n",
        "\n",
        "  model = Summarizer()\n",
        "  \n",
        "  summary = ''.join(model(body_main, min_length=60))\n",
        "  \n",
        "  r = Rouge()\n",
        "\n",
        "  [precision, recall, f1_score] = r.rouge_l([abstractL], [summary])\n",
        "\n",
        "  df = pd.DataFrame(columns=col_names,dtype=object)\n",
        "\n",
        "  df.loc[i, ['FileName']] = file_name\n",
        "  df.loc[i,['Abstract']] = abstractL\n",
        "  df.loc[i,['Summary']] =  summary\n",
        "  df.loc[i, ['Precision']] = precision\n",
        "  df.loc[i, ['Recall']] = recall\n",
        "  df.loc[i, ['F1_Score']] = f1_score\n",
        "\n",
        "  result = pd.concat([result, df], axis = 0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "43a3e097d16449fb8a428f23b7f71b41",
            "65d39dca747c4056926156fa24bec1b3",
            "98260b8ab42d4afa9e883c8b4b9bbeb8",
            "58711f66c8684f6084a0275918dc7062",
            "de43fcbc02e043c08a78a237ada1f0f8",
            "3d37b9bac1894f12ba57de36565fc9f6",
            "829d8a70fe974743b42ab83cd5fc5ced",
            "8404c3506b9d465da90e97ea3089ecfb",
            "56bd745b12ad410a89371c544032677c",
            "54264dc44faa491a8e69acb202351337",
            "fde8615944bb4ec5a189e8ddb5036b1a"
          ]
        },
        "id": "f5IKTcgGBwn-",
        "outputId": "4354f2c3-a733-4e3c-a7ef-10cb72c523f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "43a3e097d16449fb8a428f23b7f71b41"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "v4dcw8xpFQAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_csv('/content/drive/MyDrive/project/result/BERT_Extractive_Text_Summarization.csv')"
      ],
      "metadata": {
        "id": "StWZO_RyChfW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oDwgcgwx5dVI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "BERT_Extractive_Text_Summarization.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "43a3e097d16449fb8a428f23b7f71b41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_65d39dca747c4056926156fa24bec1b3",
              "IPY_MODEL_98260b8ab42d4afa9e883c8b4b9bbeb8",
              "IPY_MODEL_58711f66c8684f6084a0275918dc7062"
            ],
            "layout": "IPY_MODEL_de43fcbc02e043c08a78a237ada1f0f8"
          }
        },
        "65d39dca747c4056926156fa24bec1b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d37b9bac1894f12ba57de36565fc9f6",
            "placeholder": "",
            "style": "IPY_MODEL_829d8a70fe974743b42ab83cd5fc5ced",
            "value": "100%"
          }
        },
        "98260b8ab42d4afa9e883c8b4b9bbeb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8404c3506b9d465da90e97ea3089ecfb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_56bd745b12ad410a89371c544032677c",
            "value": 2
          }
        },
        "58711f66c8684f6084a0275918dc7062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54264dc44faa491a8e69acb202351337",
            "placeholder": "",
            "style": "IPY_MODEL_fde8615944bb4ec5a189e8ddb5036b1a",
            "value": " 2/2 [03:42&lt;00:00, 115.91s/it]"
          }
        },
        "de43fcbc02e043c08a78a237ada1f0f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d37b9bac1894f12ba57de36565fc9f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829d8a70fe974743b42ab83cd5fc5ced": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8404c3506b9d465da90e97ea3089ecfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "56bd745b12ad410a89371c544032677c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54264dc44faa491a8e69acb202351337": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fde8615944bb4ec5a189e8ddb5036b1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}